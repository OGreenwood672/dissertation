training:
  training_timesteps: 5000
  simulation_timesteps: 2000
  buffer_size: 96 # must be larger than worlds_parallised
  worlds_parallised: 24

ppo_hyperparameters:
  ppo_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_coef: 0.2
  vf_coef: 0.5
  ent_coef: 0.04

architecture:
  lstm_hidden_size: 64
