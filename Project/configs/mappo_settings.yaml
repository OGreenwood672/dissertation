training:
  training_timesteps: 321
  simulation_timesteps: 400
  timestep: 0
  buffer_size: 96 # must be larger than worlds_parallised
  worlds_parallised: 24
  seed: 15
  periodic_save_interval: 20

ppo_hyperparameters:
  ppo_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_coef: 0.2
  vf_coef: 0.5
  ent_coef: 0.01
  actor_learning_rate: 0.0001
  critic_learning_rate: 0.0001

architecture:
  lstm_hidden_size: 64
  feature_dim: 256

experiment:
  communication_type: aim
  aim_seed: 15
  vocab_size: 64
  communication_size: 8
