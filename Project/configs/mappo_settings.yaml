training:
  training_timesteps: 5000
  simulation_timesteps: 2000
  buffer_size: 96 # must be larger than worlds_parallised
  worlds_parallised: 24
  seed: -1
  periodic_save_interval: 20

ppo_hyperparameters:
  ppo_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_coef: 0.2
  vf_coef: 0.5
  ent_coef: 0.04
  learning_rate: 0.001

architecture:
  lstm_hidden_size: 64
  feature_dim: 256

experiment:
  communication_type: discrete
